package com.king.spark.streaming.upgrade.news;

import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.spark.SparkConf;
import org.apache.spark.streaming.Durations;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaInputDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka010.ConsumerStrategies;
import org.apache.spark.streaming.kafka010.KafkaUtils;
import org.apache.spark.streaming.kafka010.LocationStrategies;
import scala.Tuple2;

import java.util.Arrays;
import java.util.Collection;
import java.util.HashMap;
import java.util.Map;

/*
 * 新闻网站关键指标实时统计Spark应用程序
 * @author king
 */


public class NewsRealtimeStatSpark {

    public static void main(String[] args) throws Exception {
        // 创建Spark上下文
        SparkConf conf = new SparkConf()
                .setMaster("local[2]")
                .setAppName("NewsRealtimeStatSpark");
        JavaStreamingContext jssc = new JavaStreamingContext(
                conf, Durations.seconds(5));


        Map<String, Object> kafkaParams = new HashMap<>();
        kafkaParams.put("metadata.broker.list",
                "192.168.0.103:9092,192.168.0.104:9092");

        Collection<String> topics = Arrays.asList("news-access");

        // 创建输入DStream
        JavaInputDStream<ConsumerRecord<String, String>> stream =
                KafkaUtils.createDirectStream(
                        jssc,
                        LocationStrategies.PreferConsistent(),
                        ConsumerStrategies.Subscribe(topics, kafkaParams));

        JavaPairDStream<String, String> lines = stream.mapToPair(
                record -> new Tuple2<>(record.key(), record.value()));

        // 过滤出访问日志
        JavaPairDStream<String, String> accessDStream = lines.filter(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");

                    String action = logSplited[5];
                    if ("view".equals(action)) {
                        return true;
                    } else {
                        return false;
                    }
                });

        // 统计第一个指标：实时页面pv
        calculatePagePv(accessDStream);
        // 统计第二个指标：实时页面uv
        calculatePageUv(accessDStream);
        // 统计第三个指标：实时注册用户数
        calculateRegisterCount(lines);
        // 统计第四个指标：实时用户跳出数
        calculateUserJumpCount(accessDStream);
        // 统计第五个指标：实时版块pv
        calcualteSectionPv(accessDStream);

        jssc.start();
        jssc.awaitTermination();
        jssc.close();
    }
	
/*
	 * 计算页面pv
	 * @param
	accessDStream
*/

    private static void calculatePagePv(JavaPairDStream<String, String> accessDStream) {
        JavaPairDStream<Long, Long> pageidDStream = accessDStream.mapToPair(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");

                    Long pageid = Long.valueOf(logSplited[3]);

                    return new Tuple2<Long, Long>(pageid, 1L);
                });

        JavaPairDStream<Long, Long> pagePvDStream = pageidDStream.reduceByKey(
                (v1, v2) -> v1 + v2);

        pagePvDStream.print();

        // 在计算出每10秒钟的页面pv之后，其实在真实项目中，应该持久化
        // 到mysql，或redis中，对每个页面的pv进行累加
        // javaee系统，就可以从mysql或redis中，读取page pv实时变化的数据，以及曲线图
    }

    /*
     * 计算页面uv
     * @param <U>
     * @param accessDStream
     */

    private static <U> void calculatePageUv(JavaPairDStream<String, String> accessDStream) {
        JavaDStream<String> pageidUseridDStream = accessDStream.map(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");

                    Long pageid = Long.valueOf(logSplited[3]);
                    Long userid = Long.valueOf("null".equalsIgnoreCase(logSplited[2]) ? "-1" : logSplited[2]);

                    return pageid + "_" + userid;
                });

        JavaDStream<String> distinctPageidUseridDStream = pageidUseridDStream.transform(
                rdd -> rdd.distinct());

        JavaPairDStream<Long, Long> pageidDStream = distinctPageidUseridDStream.mapToPair(
                str -> {
                    String[] splited = str.split("_");
                    Long pageid = Long.valueOf(splited[0]);
                    return new Tuple2<Long, Long>(pageid, 1L);
                });

        JavaPairDStream<Long, Long> pageUvDStream = pageidDStream.reduceByKey((v1, v2) -> v1 + v2);

        pageUvDStream.print();
    }

    /*
     * 计算实时注册用户数
     * @param lines
     */

    private static void calculateRegisterCount(JavaPairDStream<String, String> lines) {
        JavaPairDStream<String, String> registerDStream = lines.filter(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");

                    String action = logSplited[5];
                    if ("register".equals(action)) {
                        return true;
                    } else {
                        return false;
                    }
                });

        JavaDStream<Long> registerCountDStream = registerDStream.count();

        registerCountDStream.print();

        // 每次统计完一个最近10秒的数据之后，不是打印出来
        // 去存储（mysql、redis、hbase），选用哪一种主要看你的公司提供的环境，以及你的看实时报表的用户以及并发数量，包括你的数据量
        // 如果是一般的展示效果，就选用mysql就可以
        // 如果是需要超高并发的展示，比如QPS 1w来看实时报表，那么建议用redis、memcached
        // 如果是数据量特别大，建议用hbase

        // 每次从存储中，查询注册数量，最近一次插入的记录，比如上一次是10秒前
        // 然后将当前记录与上一次的记录累加，然后往存储中插入一条新记录，就是最新的一条数据
        // 然后javaee系统在展示的时候，可以比如查看最近半小时内的注册用户数量变化的曲线图
        // 查看一周内，每天的注册用户数量的变化曲线图（每天就取最后一条数据，就是每天的最终数据）
    }

    /*
     * 计算用户跳出数量
     * @param accessDStream
     */

    private static void calculateUserJumpCount(JavaPairDStream<String, String> accessDStream) {
        JavaPairDStream<Long, Long> useridDStream = accessDStream.mapToPair(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");
                    Long userid = Long.valueOf("null".equalsIgnoreCase(logSplited[2]) ? "-1" : logSplited[2]);
                    return new Tuple2<Long, Long>(userid, 1L);
                });

        JavaPairDStream<Long, Long> useridCountDStream = useridDStream.reduceByKey(
                (v1, v2) -> v1 + v2);

        JavaPairDStream<Long, Long> jumpUserDStream = useridCountDStream.filter(
                tuple -> {
                    if (tuple._2 == 1) {
                        return true;
                    } else {
                        return false;
                    }
                });

        JavaDStream<Long> jumpUserCountDStream = jumpUserDStream.count();

        jumpUserCountDStream.print();
    }

    /*
     * 版块实时pv
     * @param accessDStream
     */

    private static void calcualteSectionPv(JavaPairDStream<String, String> accessDStream) {
        JavaPairDStream<String, Long> sectionDStream = accessDStream.mapToPair(
                tuple -> {
                    String log = tuple._2;
                    String[] logSplited = log.split(" ");

                    String section = logSplited[4];

                    return new Tuple2<String, Long>(section, 1L);
                });

        JavaPairDStream<String, Long> sectionPvDStream = sectionDStream.reduceByKey(
                (v1, v2) -> v1 + v2);

        sectionPvDStream.print();
    }

}
